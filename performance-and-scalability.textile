Improving performance can increase complexity, increasing the likelihood of safety and liveness failures.

First, make your program right, then if performance requirements tell you so make it faster.

h2. Thinking about performance

h4. Performance and scalability

*Resources*: CPU cycles, memory, network bandwidth, IO bandwidth, database requests, disk space, etc.
When the performance of the activity is limited by availability of a particular resource, we say it is *bounded by that resource*: CPU-bound, for instance. Performance is discussed in the context of the resource.

Using threads always introduces some performance costs compared to the single-threaded approach:
* overhead associated with coordinating between threads (locking, signaling, memory synchronization)
* increased context switching
* thread creation and teardown
* scheduling overhead

Two ways to measure application performance
* *how fast* a given unit of work can be processed
** service time, latency
** area of *traditional performance optimizations*: caching, using optimal algorithms
* *how much* work can be performed with a given quantity of computing resources
** capacity, throughput
** area of *scalability optimizations*

These two aspects of performance are completely separate, sometimes even at odds with each other
* Scalability is usually of greater concerns for server applications
* For interactive applications lateny aspetcs (traditional performance) tend to be more important

h4. Evaluating performance tradeoffs

Avoid premature optimizations. First make it right, then make it fast -- if it is not already fast enough.

Then *use conservative approach* in tuning performance
* What do you mean by faster?
* Under which conditions will this approach be actually faster? Light or heavy load? Large or small data sets? Support the answer with measurements.
* How often are these conditions likely to arise in your situation? Support the answer with measurements.
* Is the code likely to be used in other situations where the conditions may be different?
* What hidden costs are you trading for this improved performance? Is this a good tradeoff?

*Why you should take conservative approach* in tuning performance:
* The quest for performance is probably the single greatest source of concurrency bugs which are amongh the most difficult bugs to track down and eliminate. Trade carefully.
* Worse, trading something for performance you can get neither.

*Tradeoffs*:
* one cost for another (service time versus memory consumtion)
* trading cost for safety
* trading cost for readability or maintainability
** clever or non-obvious code
** compromising good object-oriented design principles, such as breaking encapsulation
** risk of error, faster algorithms are more complicated

*Measure, never guess.* Always support your reasoning about performance with measurements: the intuition of where the performance problem lies is often incorrect.

h2. Amdahl's law

*Speedup <= 1 / (F + (1 - F) / N)*
where
* *F* in [0; 1] is the fraction of calculation that must be executed serially
* *1 - F* is the fraction of calculation that can be executed in parllel
* *N* is the amount of processors
* *(1 - F) / N* is the time that *1 - F* part of calculation takes on *N* processors

If *N* is infiity
* If *F* is 0,5 then the maximum speedup converges to 1/F, meaning that such a program can be sped up by a factor of 2
* If *F* is 0,1 then the program can be sped up by a factor of 10

*Utilization* is the *speedup* divided by the number of resource *N*.
* Perfectly, utilization is 1, meaning *N* added reousrces produce speedup=N, and N/N=1
* If you add N resources and get speedup=P<N then you've actually utilized P/N of added resources, hence term *utilization*