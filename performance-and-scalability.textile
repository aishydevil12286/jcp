Improving performance can increase complexity, increasing the likelihood of safety and liveness failures.

First, make your program right, then if performance requirements tell you so make it faster.

h2. Thinking about performance

h4. Performance and scalability

*Resources*: CPU cycles, memory, network bandwidth, IO bandwidth, database requests, disk space, etc.
When the performance of the activity is limited by availability of a particular resource, we say it is *bounded by that resource*: CPU-bound, for instance. Performance is discussed in the context of the resource.

Using threads always introduces some performance costs compared to the single-threaded approach:
* overhead associated with coordinating between threads (locking, signaling, memory synchronization)
* increased context switching
* thread creation and teardown
* scheduling overhead

Two ways to measure application performance
* *how fast* a given unit of work can be processed
** service time, latency
** area of *traditional performance optimizations*: caching, using optimal algorithms
* *how much* work can be performed with a given quantity of computing resources
** capacity, throughput
** area of *scalability optimizations*

These two aspects of performance are completely separate, sometimes even at odds with each other
* Scalability is usually of greater concerns for server applications
* For interactive applications lateny aspetcs (traditional performance) tend to be more important

h4. Evaluating performance tradeoffs

Avoid premature optimizations. First make it right, then make it fast -- if it is not already fast enough.

Then *use conservative approach* in tuning performance
* What do you mean by faster?
* Under which conditions will this approach be actually faster? Light or heavy load? Large or small data sets? Support the answer with measurements.
* How often are these conditions likely to arise in your situation? Support the answer with measurements.
* Is the code likely to be used in other situations where the conditions may be different?
* What hidden costs are you trading for this improved performance? Is this a good tradeoff?

*Why you should take conservative approach* in tuning performance:
* The quest for performance is probably the single greatest source of concurrency bugs which are amongh the most difficult bugs to track down and eliminate. Trade carefully.
* Worse, trading something for performance you can get neither.

*Tradeoffs*:
* one cost for another (service time versus memory consumtion)
* trading cost for safety
* trading cost for readability or maintainability
** clever or non-obvious code
** compromising good object-oriented design principles, such as breaking encapsulation
** risk of error, faster algorithms are more complicated

*Measure, never guess.* Always support your reasoning about performance with measurements: the intuition of where the performance problem lies is often incorrect.

h2. Amdahl's law

*Speedup <= 1 / (F + (1 - F) / N)*
where
* *F* in [0; 1] is the fraction of calculation that must be executed serially
* *1 - F* is the fraction of calculation that can be executed in parllel
* *N* is the amount of processors
* *(1 - F) / N* is the time that *1 - F* part of calculation takes on *N* processors

*Utilization* is the *speedup* divided by the number of resources *N*.
* Perfectly, utilization is 1, meaning *N* added reousrces produce speedup=N, and N/N=1
* If you add N resources and get speedup=P<N then you've actually utilized P/N of added resources, hence the term *utilization*

The serial fraction of computaion must be identified carefully
* It can include time to fetch data being processed from queue/dequeue (less for non-blocking lists)
* It must include some time for result handling. Time to merge the results or for producing a side-effect.

h4. Applying Amdahl's law qualitively

The law quantifies the possible speedup if we can accurately estimate the fraction of execution that is serialized. Also, this law can be used without such measurement.

When evaluating an algorithm, thinking "in the limit" about what would happen with hundreds or thousands of processors can offer some insight into where scaling might appear.

h2. Costs introduced by threads

For threads to offer a performance improvement, the performance benefits of parallelization must outweight the costs introduced by concurrency.

h4. Context switching

If there are more than one runnable threads than CPUs, eventually the OS will preempt one thread to start another. This causes a context switch, which requires
* saving the execution context of the currently running thread
* restoring the execution context of the newly scheduled thread

Why context switches are not free
* Thread scheduling requires manipulating shared data structures in the OS and JVM. This activiy takes CPU time.
* When a new thread is switched in, the data it needs is unlikely to be in the local processor cache, so a context switch causes a flurry of cache misses, threads run a little more slowly when they are first scheduled. (This is why schedulers give threads some minimum time quantum to amortize the cose of the context switch, improving overall throughput.)
* When a thread blocks (waiting for a contended lock, blocking on IO) it is suspended by the JVM and is allowed to be switched out. If the thread blocks frequently, it will be unable to use their full scheduling quantum.

*vmstat* Unix tool shows the number of context switches and the percentage of time spent in the kernel. High kernel usage indicates heavy scheduling activity, which may be caused by blocking due to IO or lock contention.

h4. Memory synchronization

Sources:
* Visibility guarantees provided by *synchronized* and *volatile* may entail using special instructions called memory barriers that can flush or invalidate caches, flush hardware write buffers, stall execution pipelines.
* Indirect performance consequences because they inhibit other compiler optimizations, for examples, many operations can not be reordered with memory barriers.
* Synchronization creates traffic on the shared memory bus which has a limited bandwidth and is shared across all processors. Threads start competing for synchronization bandwidth, all threads using synchronization will suffer.

It is important to distinguish between *contended* and *uncontended* synchronizations, because *synchronized* mechanism is optimized for the *uncontended* case (*volatile* is always *unconteded*). Focues optimization efforts on areas where lock contention actually occurs.
* The performance cost of a "fast-path" uncontended synchronization ranges from 50 to 250 clock cycles, which is rarely significant in overall application performance, and the alternative involves compromising safety.
* JVMs can reduce the cost of incidental syncrhonization by optimizaing away locking that can be proven to never contend. For examples, when a lock object is accessible only to the current thread.
* JVMs can use escape analysis to identify when a local object reference is never published to the heap. All lock acquisitions can be eliminated.
* Compilers can perform lock coarsening, the merging of adjacent *synchronized* blocks using the same lock. This may also enable more optimizations inside the larger block of code.

h4. Blocking

When locking is *contended*, the losing thread must block.

Blocking can be implemented via spin-waiting or by suspending the blocked thread through the OS. Spin-waiting is preferable for short waits and suspension is preferable for long waits. Some JVMs choose adaptively but most suspend a thread.

Suspending a thread entails two additional context switches:
* the blocked thread is switched out before its quantum is expired
* swithced back when the lock or resource is available
* blocking due to the lock contention: when a thread releases the lock it must then ask the OS to resume the blocked thread

h2. Reducing lock contention

*Lock contention* causes both *serialization* and *context switches*
* *Serialization* hurts *scalability*
* *Context switches* hurt hurts *performance*

The corollary of Little's law, a result from queueing theory, says *"the average number of customers in a stable system is equal to their average arrival rate multiplied by their average time in the system".*

Hence 3 ways to reduce lock contention:
* Reduce the duration for which locks are held
* Reduce the frequency with which locks are requested
* Replace exclusive locks with coordination mechanisms that permit greater concurrency

h4. Narrowing lock scope

Hold locks as briefly as possible:
* move code that does not require the lock out of *synchronized* block
* especially for expensive operations and potentially blocking operations such as I/O.

h4. Reducing lock granularity

Reducing the time the lock is held -- have threads to ask for it less often
* *lock splittiing*
* *lock striping*

The disadvantage is that the risk of deadlock increases.

*Splitting locks*:
* offers greatest possibility for improvement when the lock experiences moderate but not heavy contention, might actually turn them into mostly uncontended locks
* for locks experiencing little contention this yields little improvement in performance or throughput, but might increase the load threshold at which performance starts to degrade due to contention

Splitting a heavily contended lock into two is likely to result in two heavily contended locks. Lock splitting can be extended to *lock striping* -- partitioning locking on a variable-sized set of independent objects. (Implementation of *ConcurrentHashMap* uses an array of 16 locks, each of which guards 1/16 of the hash buckets).

The disadvantage of *striping locks* is that locking collection for exclusive access is more difficult and costly than with a single lock, all locks must be acquired.

h4. Avoiding hot fields

h4. Alternatives to exclusive locks

h4. Monitoring CPU utilization

h4. Objects pooling

h2. Comparing map performance

h2. Reducing context switch overhead

